{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOFaeV+ViwyoBtP8fPzP3eZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qhtJv7dT234F"},"outputs":[],"source":["# In this notebook we will use compacted images"]},{"cell_type":"code","source":["!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","\n","!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGiI3pKM3Hm3","executionInfo":{"status":"ok","timestamp":1718471539742,"user_tz":-330,"elapsed":11722,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"72541252-1b17-446a-db58-11bec03739ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-15 17:12:06--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.175.207, 74.125.24.207, 142.251.10.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.175.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘horse-or-human.zip’\n","\n","horse-or-human.zip  100%[===================>] 142.65M  19.6MB/s    in 8.9s    \n","\n","2024-06-15 17:12:15 (16.0 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n","\n","--2024-06-15 17:12:15--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.175.207, 74.125.24.207, 142.251.10.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.175.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘validation-horse-or-human.zip’\n","\n","validation-horse-or 100%[===================>]  10.95M  6.23MB/s    in 1.8s    \n","\n","2024-06-15 17:12:17 (6.23 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"]}]},{"cell_type":"code","source":["import zipfile\n","\n","local_zip = './horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./horse-or-human')\n","zip_ref.close()\n","\n","local_zip = './validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./validation-horse-or-human')\n","zip_ref.close()"],"metadata":{"id":"eCKm5sK-3M1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Directory with our training horse pictures\n","train_horse_dir = os.path.join('./horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","train_human_dir = os.path.join('./horse-or-human/humans')\n","\n","# Directory with validation horse pictures\n","validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n","\n","# Directory with validation human pictures\n","validation_human_dir = os.path.join('./validation-horse-or-human/humans')"],"metadata":{"id":"JDmlxrpB3SHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_horse_names = os.listdir(train_horse_dir)\n","print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n","\n","train_human_names = os.listdir(train_human_dir)\n","print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n","\n","validation_horse_names = os.listdir(validation_horse_dir)\n","print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n","\n","validation_human_names = os.listdir(validation_human_dir)\n","print(f'VAL SET HUMANS: {validation_human_names[:10]}')\n","\n","print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n","print(f'total training human images: {len(os.listdir(train_human_dir))}')\n","print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n","print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSUFYi103Y5c","executionInfo":{"status":"ok","timestamp":1718471541902,"user_tz":-330,"elapsed":8,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"3a6298b3-112c-41c6-af7d-ebbb432241ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN SET HORSES: ['horse08-2.png', 'horse23-5.png', 'horse44-7.png', 'horse05-5.png', 'horse05-1.png', 'horse20-7.png', 'horse30-6.png', 'horse49-8.png', 'horse27-0.png', 'horse01-8.png']\n","TRAIN SET HUMANS: ['human15-30.png', 'human06-15.png', 'human11-28.png', 'human13-07.png', 'human17-00.png', 'human06-19.png', 'human17-22.png', 'human07-29.png', 'human15-23.png', 'human01-23.png']\n","VAL SET HORSES: ['horse5-514.png', 'horse4-188.png', 'horse5-504.png', 'horse2-218.png', 'horse4-000.png', 'horse4-495.png', 'horse2-136.png', 'horse4-468.png', 'horse1-411.png', 'horse4-501.png']\n","VAL SET HUMANS: ['valhuman05-18.png', 'valhuman01-22.png', 'valhuman03-08.png', 'valhuman02-17.png', 'valhuman02-00.png', 'valhuman01-18.png', 'valhuman02-13.png', 'valhuman01-10.png', 'valhuman02-01.png', 'valhuman04-02.png']\n","total training horse images: 500\n","total training human images: 527\n","total validation horse images: 128\n","total validation human images: 128\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    # tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n","    # tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Flatten(),\n","\n","    tf.keras.layers.Dense(512, activation = 'relu'),\n","    tf.keras.layers.Dense(1, activation = 'sigmoid')\n","])"],"metadata":{"id":"d5asI6AX3boo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPq_Q0wR3isr","executionInfo":{"status":"ok","timestamp":1718471541902,"user_tz":-330,"elapsed":7,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"536a8d76-a60e-4ff4-d583-94cc12b2f26b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 148, 148, 16)      448       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 74, 74, 16)        0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 17, 17, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 18496)             0         \n","                                                                 \n"," dense (Dense)               (None, 512)               9470464   \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 9494561 (36.22 MB)\n","Trainable params: 9494561 (36.22 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(learning_rate=0.001),\n","              metrics=['accuracy'])"],"metadata":{"id":"FfIqoZ1r34dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        './horse-or-human/',  # This is the source directory for training images\n","        target_size=(150, 150),  # All images will be resized to 150x150\n","        batch_size=128,\n","        # Since you used binary_crossentropy loss, you need binary labels\n","        class_mode='binary')\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","        './validation-horse-or-human/',  # This is the source directory for training images\n","        target_size=(150, 150),  # All images will be resized to 150x150\n","        batch_size=32,\n","        # Since you used binary_crossentropy loss, you need binary labels\n","        class_mode='binary')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5GMyMK34fqH","executionInfo":{"status":"ok","timestamp":1718471563863,"user_tz":-330,"elapsed":1129,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"8fa22c31-5736-4a8a-a014-e31e431a9ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1027 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["history = model.fit(\n","      train_generator,\n","      steps_per_epoch=8,\n","      epochs=15,\n","      verbose=1,\n","      validation_data = validation_generator,\n","      validation_steps=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDRgdtPi4xIa","executionInfo":{"status":"ok","timestamp":1718471704996,"user_tz":-330,"elapsed":128017,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"5dc3ef0a-a8b7-42a0-d2da-68b0a9a82adb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","8/8 [==============================] - 12s 754ms/step - loss: 1.3369 - accuracy: 0.5451 - val_loss: 0.6524 - val_accuracy: 0.5156\n","Epoch 2/15\n","8/8 [==============================] - 6s 812ms/step - loss: 0.5422 - accuracy: 0.7734 - val_loss: 0.4089 - val_accuracy: 0.8398\n","Epoch 3/15\n","8/8 [==============================] - 7s 906ms/step - loss: 0.6084 - accuracy: 0.7887 - val_loss: 0.6403 - val_accuracy: 0.5977\n","Epoch 4/15\n","8/8 [==============================] - 7s 833ms/step - loss: 0.3212 - accuracy: 0.8809 - val_loss: 0.9526 - val_accuracy: 0.6875\n","Epoch 5/15\n","8/8 [==============================] - 7s 1s/step - loss: 0.2201 - accuracy: 0.9055 - val_loss: 0.9620 - val_accuracy: 0.7656\n","Epoch 6/15\n","8/8 [==============================] - 7s 903ms/step - loss: 0.1515 - accuracy: 0.9499 - val_loss: 3.3210 - val_accuracy: 0.5039\n","Epoch 7/15\n","8/8 [==============================] - 7s 836ms/step - loss: 0.3877 - accuracy: 0.8687 - val_loss: 0.7800 - val_accuracy: 0.7773\n","Epoch 8/15\n","8/8 [==============================] - 8s 982ms/step - loss: 0.1060 - accuracy: 0.9600 - val_loss: 1.0221 - val_accuracy: 0.8164\n","Epoch 9/15\n","8/8 [==============================] - 6s 776ms/step - loss: 0.0430 - accuracy: 0.9889 - val_loss: 1.1809 - val_accuracy: 0.8242\n","Epoch 10/15\n","8/8 [==============================] - 6s 782ms/step - loss: 0.0372 - accuracy: 0.9878 - val_loss: 1.2523 - val_accuracy: 0.8320\n","Epoch 11/15\n","8/8 [==============================] - 7s 937ms/step - loss: 0.1667 - accuracy: 0.9422 - val_loss: 0.3693 - val_accuracy: 0.8945\n","Epoch 12/15\n","8/8 [==============================] - 6s 772ms/step - loss: 0.0562 - accuracy: 0.9789 - val_loss: 1.1786 - val_accuracy: 0.8203\n","Epoch 13/15\n","8/8 [==============================] - 7s 915ms/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 1.3269 - val_accuracy: 0.8086\n","Epoch 14/15\n","8/8 [==============================] - 7s 827ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 1.4316 - val_accuracy: 0.8281\n","Epoch 15/15\n","8/8 [==============================] - 7s 909ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 1.4558 - val_accuracy: 0.8359\n"]}]},{"cell_type":"code","source":["# Code to make predictions in colab\n","\n","import numpy as np\n","\n","from google.colab import files\n","from tensorflow.keras.utils import load_img, img_to_array\n","\n","uploaded=files.upload()\n","\n","for fn in uploaded.keys():\n","\n","  # predicting images\n","  path='/content/' + fn\n","  img=load_img(path, target_size=(150, 150))\n","\n","  x=img_to_array(img)\n","  x /= 255\n","  x=np.expand_dims(x, axis=0)\n","  images = np.vstack([x])\n","\n","  classes = model.predict(images, batch_size=10)\n","\n","  print(classes[0])\n","\n","  if classes[0]>0.5:\n","    print(fn + \" horse\")\n","  else:\n","    print(fn + \" human\")\n",""],"metadata":{"id":"ksEvpg6k40ge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"51PeICyTfnV0"},"execution_count":null,"outputs":[]}]}