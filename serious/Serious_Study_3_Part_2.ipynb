{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNucn+2P8BWrQda6KiO+JXS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["***We will add validation set in this notebook***"],"metadata":{"id":"qAeTH2LQzFKR"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_mJx04ArjvB","executionInfo":{"status":"ok","timestamp":1718470114985,"user_tz":-330,"elapsed":1396,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"54ca20db-ab37-4cec-f567-1fe98ec92a6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-15 16:48:32--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 74.125.137.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘horse-or-human.zip.1’\n","\n","horse-or-human.zip. 100%[===================>] 142.65M   275MB/s    in 0.5s    \n","\n","2024-06-15 16:48:32 (275 MB/s) - ‘horse-or-human.zip.1’ saved [149574867/149574867]\n","\n","--2024-06-15 16:48:32--  https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 74.125.137.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘validation-horse-or-human.zip’\n","\n","validation-horse-or 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n","\n","2024-06-15 16:48:33 (81.2 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"]}],"source":["!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n","\n","!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"]},{"cell_type":"code","source":["import zipfile\n","\n","local_zip = './horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./horse-or-human')\n","zip_ref.close()\n","\n","local_zip = './validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('./validation-horse-or-human')\n","zip_ref.close()"],"metadata":{"id":"owoo_puPsMCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Directory with our training horse pictures\n","train_horse_dir = os.path.join('./horse-or-human/horses')\n","\n","# Directory with our training human pictures\n","train_human_dir = os.path.join('./horse-or-human/humans')\n","\n","# Directory with validation horse pictures\n","validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n","\n","# Directory with validation human pictures\n","validation_human_dir = os.path.join('./validation-horse-or-human/humans')"],"metadata":{"id":"ulitTecgswW4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_horse_names = os.listdir(train_horse_dir)\n","print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n","\n","train_human_names = os.listdir(train_human_dir)\n","print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n","\n","validation_horse_names = os.listdir(validation_horse_dir)\n","print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n","\n","validation_human_names = os.listdir(validation_human_dir)\n","print(f'VAL SET HUMANS: {validation_human_names[:10]}')\n","\n","print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n","print(f'total training human images: {len(os.listdir(train_human_dir))}')\n","print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n","print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojG0ceuns53U","executionInfo":{"status":"ok","timestamp":1718470236612,"user_tz":-330,"elapsed":603,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"f704870a-d300-4e0d-915d-c6b573f971a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN SET HORSES: ['horse08-2.png', 'horse5-514.png', 'horse23-5.png', 'horse44-7.png', 'horse05-5.png', 'horse05-1.png', 'horse20-7.png', 'horse4-188.png', 'horse30-6.png', 'horse49-8.png']\n","TRAIN SET HUMANS: ['human15-30.png', 'human06-15.png', 'human11-28.png', 'valhuman05-18.png', 'valhuman01-22.png', 'human13-07.png', 'human17-00.png', 'human06-19.png', 'human17-22.png', 'valhuman03-08.png']\n","VAL SET HORSES: ['horse5-514.png', 'horse4-188.png', 'horse5-504.png', 'horse2-218.png', 'horse4-000.png', 'horse4-495.png', 'horse2-136.png', 'horse4-468.png', 'horse1-411.png', 'horse4-501.png']\n","VAL SET HUMANS: ['valhuman05-18.png', 'valhuman01-22.png', 'valhuman03-08.png', 'valhuman02-17.png', 'valhuman02-00.png', 'valhuman01-18.png', 'valhuman02-13.png', 'valhuman01-10.png', 'valhuman02-01.png', 'valhuman04-02.png']\n","total training horse images: 628\n","total training human images: 655\n","total validation horse images: 128\n","total validation human images: 128\n"]}]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"ppdDAc35tAK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    tf.keras.layers.Flatten(),\n","\n","    tf.keras.layers.Dense(512, activation = 'relu'),\n","    tf.keras.layers.Dense(1, activation = 'sigmoid')\n","])"],"metadata":{"id":"kOn0w9IItFEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wg6tW2ttuQYF","executionInfo":{"status":"ok","timestamp":1718470261251,"user_tz":-330,"elapsed":5,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"807e8f85-d067-4c56-b914-dd2a0fe79d67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_8 (Conv2D)           (None, 298, 298, 16)      448       \n","                                                                 \n"," max_pooling2d_8 (MaxPoolin  (None, 149, 149, 16)      0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 147, 147, 32)      4640      \n","                                                                 \n"," max_pooling2d_9 (MaxPoolin  (None, 73, 73, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 71, 71, 64)        18496     \n","                                                                 \n"," max_pooling2d_10 (MaxPooli  (None, 35, 35, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 33, 33, 64)        36928     \n","                                                                 \n"," max_pooling2d_11 (MaxPooli  (None, 16, 16, 64)        0         \n"," ng2D)                                                           \n","                                                                 \n"," flatten_2 (Flatten)         (None, 16384)             0         \n","                                                                 \n"," dense_4 (Dense)             (None, 512)               8389120   \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 8450145 (32.23 MB)\n","Trainable params: 8450145 (32.23 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import RMSprop\n","\n","model.compile(\n","    loss = 'binary_crossentropy',\n","    optimizer = RMSprop(learning_rate = 0.001),\n","    metrics = ['accuracy']\n",")"],"metadata":{"id":"7MTCm_00umNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_gen = ImageDataGenerator(rescale = 1/255)\n","validation_gen = ImageDataGenerator(rescale=1/255)\n","\n","\n","train_generator = train_gen.flow_from_directory(\n","    './horse-or-human',\n","    target_size = (300, 300),\n","    batch_size = 128,\n","    class_mode = 'binary'\n",")\n","\n","\n","validation_generator = validation_gen.flow_from_directory(\n","    './validation-horse-or-human',\n","    target_size = (300, 300),\n","    batch_size = 32,\n","    class_mode = 'binary'\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qgR6Bsmu_My","executionInfo":{"status":"ok","timestamp":1718470374213,"user_tz":-330,"elapsed":686,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"d3ff1849-e63b-4d85-d1a0-7a86869d9b04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1283 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["model.fit(\n","    train_generator,\n","    steps_per_epoch = 8,\n","    epochs = 15,\n","    verbose = 1,\n","    validation_data = validation_generator,\n","    validation_steps = 8\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIJY7YOrvtUe","executionInfo":{"status":"ok","timestamp":1718470604486,"user_tz":-330,"elapsed":159524,"user":{"displayName":"gaurav sharma","userId":"14938895319457339719"}},"outputId":"0bf0eb8a-db20-4c50-d3dc-f94852099cd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","8/8 [==============================] - 10s 1s/step - loss: 1.1889 - accuracy: 0.5061 - val_loss: 0.6936 - val_accuracy: 0.5000\n","Epoch 2/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.6688 - accuracy: 0.6374 - val_loss: 0.7007 - val_accuracy: 0.5000\n","Epoch 3/15\n","8/8 [==============================] - 10s 1s/step - loss: 1.3050 - accuracy: 0.6229 - val_loss: 0.6454 - val_accuracy: 0.5820\n","Epoch 4/15\n","8/8 [==============================] - 12s 2s/step - loss: 0.6632 - accuracy: 0.7453 - val_loss: 0.4821 - val_accuracy: 0.8828\n","Epoch 5/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.4922 - accuracy: 0.8065 - val_loss: 0.9695 - val_accuracy: 0.5039\n","Epoch 6/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.5262 - accuracy: 0.7529 - val_loss: 0.3468 - val_accuracy: 0.8672\n","Epoch 7/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.2717 - accuracy: 0.9110 - val_loss: 0.4898 - val_accuracy: 0.8359\n","Epoch 8/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.2818 - accuracy: 0.9043 - val_loss: 0.2386 - val_accuracy: 0.9062\n","Epoch 9/15\n","8/8 [==============================] - 12s 1s/step - loss: 0.1975 - accuracy: 0.9355 - val_loss: 0.2263 - val_accuracy: 0.9258\n","Epoch 10/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.1749 - accuracy: 0.9365 - val_loss: 0.2103 - val_accuracy: 0.9219\n","Epoch 11/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.7728 - accuracy: 0.7953 - val_loss: 0.4003 - val_accuracy: 0.8359\n","Epoch 12/15\n","8/8 [==============================] - 11s 1s/step - loss: 0.1977 - accuracy: 0.9297 - val_loss: 0.3657 - val_accuracy: 0.8984\n","Epoch 13/15\n","8/8 [==============================] - 9s 1s/step - loss: 0.1639 - accuracy: 0.9422 - val_loss: 0.4732 - val_accuracy: 0.8555\n","Epoch 14/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.1910 - accuracy: 0.9297 - val_loss: 0.1336 - val_accuracy: 0.9453\n","Epoch 15/15\n","8/8 [==============================] - 10s 1s/step - loss: 0.0862 - accuracy: 0.9727 - val_loss: 0.1865 - val_accuracy: 0.9453\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7c662c5df0d0>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":[],"metadata":{"id":"3dZAkng1wvU4"},"execution_count":null,"outputs":[]}]}